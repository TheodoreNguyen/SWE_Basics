CLRS:
    
I Foundations
    Introduction 3
    1 The Role of Algorithms in Computing 5
        1.1 Algorithms 5
            - what is an ALGORITHM - procedure that takes a specific input and gives a 
                specific output to solve a problem
            - SORTING PROBLEM: LTX
                Input: sequence of n numbers (a1, a2, .., an)
                Output: a permutation/reordering of input sequence (a1', a2', ..., an')
                    such that a1' <= a2' <= ... <= an'
            - an INSTANCE of a problem is one example input to said problem
            - an algorithm is CORRECT if for every possible existing input instance, it
                halts having generated the correct output 
            - most problems have (1) many candidate solns, making it hard to see which one
                is the 'best' one, (2) have solns with practical applications
            - a DATA STRUCTURE is a way to store & organize data to facilitate access/mods
            - algorithms isn't just about knowing known algos - its about technique
            - really hard problems are don't have an efficient solution today
                - a subset of these are called NP-complete
                    - nobody has proven that an efficient soln to these CANT exist
                        - if an efficient soln exists for one, then it exists for ALL 
            - PARALLELISM run stuff on multiple cores cuz we cant incresase clock speed. 
        1.2 Algorithms as a technology 11
            - if we had infinite SPEED, then we still need to design CORRECTNESS
            - algorithms whose running time grows more slowly w.r.t. data size are much
                faster as the data size increases
            - algorithms are a technology too. every technology is based on algorithms
        
        
    2 Getting Started 16
        - examples. Algorithms are written in PSUEDOCODE
        2.1 Insertion sort 16
            - solves the SORTING PROBLEM, the values of the seq we sort are called KEYS
            - its how humans sort (KINDA)! pick up some cards and sort them =)
                - go from left to right and move things to the left as you go along
            - the index J represents the current thing/card being sorted
            - at any iteration, subarray A[1:j-1] is the currently sorted array
                - the elements at A[1:j-1] were originally there too, but just NOW they're
                    in sorted order
            - A[j] is compared to A[j-1], and if smaller, A[k:j-1] is shifted one element
                right, overwriting A[j], where A[k-1] is less than original A[j]. So
                original A[j] takes up A[k]'s spot. 
            - done for j= 2nd element to the end of the array.
                    
            O(N) best case, O(N^2) worst case        
            INSERTION_SORT(A):
                for j=2 to length(A)
                    key = A[j]
                    //insert A[j] into sorted sequence A[1..j-1]
                    i = j - 1
                    while i > 0 and A[i] > key
                        A[i + 1] = A[i]
                        i = i -1
                    A[i + 1] = key
                    
            def insertion_sort(A):
                for j in range(1,len(A)):
                    key = A[j]
                    i = j - 1
                    while(i >= 0 and A[i] > key):
                        A[i + 1] = A[i]
                        i -= 1
                    A[i + 1] = key
                return A
https://upload.wikimedia.org/wikipedia/commons/4/42/Insertion_sort.gif
                    
            def insertion_sort_intuitive_ACTUALLY_BUBBLESORT(A):
                for i in range(1, len(A)):
                    for j in range(1, len(A)):
                        temp = A[j]
                        if A[j] < A[j-1]:
                            A[j] = A[j-1]
                            A[j-1] = temp
                return A
            
            - LOOP INVARIANT for insertion sort: at the start of any iteration of the loop
            for insertion sort, the subarray A[1..j-1] consists of the elements 
            originally in A[1..j-1] but now in sorted order.
            
            - Show three things about loop inavariants
                (1) initialization: True prior to first iteration of loop
                (2) maintenance: if true before an iteration of a loop, remains true before
                    next iteration. 
                        - (1) + (2) is like PMI
                (3) termination: invariant gives useful property at loop end to help show
                    algorithm in correct
                        - PMI ends when loop terminate instead of infinitely like induction
            
            - insertion sort loop invariant proof
                (1) A[1..j-1] before the loop starts is just A[1]. A[1] is the original
                    A[1] and is in sorted order, trivially shown (since its one element)
                (2) RV? dont need rigor 
                (3) at the end of the loop, j = n + 1. By the loop invariant, then 
                    A[1..j-1] == A[1..(n+1)-1] == A[1..n] must be in sorted order - proving
                    the loop invariant correctly
            
            - psuedocode conventions - read the section if you want.
            
            
        2.2 Analyzing algorithms 23
            - INPUT SIZE depends on the problem - usually it is N=number of input items
            - RUNNING TIME of an algorithm on an instance is the number of primtive
                operations/steps executed. We only care about WORST-CASE running time 
                LTX
                
                T(N) = c1*k1 + c2*k2 + ... + cl*kl
                
                is the runtime of a specific algorithm, where, for i = 1..l
                ci is the cost of running that particular line once
                ki is the number of times this line is run
                l is the total number lines of code
                    each term represents the cost of that line of code.
                    
            - only care about RATE OF GROWTH or ORDER OF GROWTH of running time, which is
                why we drop constants since the rate of growth only depends on N
                - rate efficiency off of growth rate.
                
        2.3 Designing algorithms 29
            approaches include
            - INCREMENTAL, RECURSIVE, etc. The middle usually follows DIVIDE AND CONQUER
                (1) DIVIDE the problem into subproblems that are smaller instances of the
                    same problem
                (2) CONQUER the subproblems by solving them recursively. (aka, divide and
                    conquer them as well). If small enough, solve them straightforward 
                (3) COMBINE solutions to all subproblems into solution to original probblem
            - MERGE SORT is a divide and conqyuer algorithm
                (1) DIVIDE the n-element sequence into subsequences of n/2-elements each
                (2) CONQUER the two subsequences recursively via merge sort
                (3) COMBINE the two sorted subsequences to get the sorted original sequence
                    - recursion here bottoms out in base case when subsequence length = 1
                    - combine (MERGE) step is the most important for mergesort
                    
                    O(N) speed for merge
                    
                    MERGE(A, p, q, r)
                        - A is concat of merged subarrays - p <= q < random
                        - A[p..q] and A[q+1..r] are both sorted
                        - returns sorted subarray A[p..r] of length n = r - p + 1
                        - literally just goes through each sorted array from bottom to top
                            comparing the current 'bottom' elements and adding them to the 
                            final array after comparisons. 
                            - has a sentinel node so we don't have to check for emptiness
                            
                    MERGE(A, p, q, r)
                        n1 = q - p + 1
                        n2 = r - q
                        L[1..n1 + 1] be new array of length n1
                        R[1..n2 + 1] be new array of length n2
                        for i = 1 to n1
                            L[i] = A[p + i - 1]
                        for j = 1 to n2 
                            R[j] = A[q + j]
                        L[n1 + 1] = infinity
                        R[n2 + 1] = infinity
                        i = 1
                        j = 1
                        for k = p..r
                            if L[i] <= R[j]
                                A[k] = L[i]
                                i = i + 1
                            else
                                A[k] = R[j]
                                j = j + 1
                    - NOTE that MERGE doesn't operate in place
                    - book uses concept of SENTINEL or DUMMY indicator 'NODE'
                        
                    from copy import copy
                    def merge(A, p, q, r):
                        L = copy(A[p:q+1])
                        R = copy(A[q+1:r+1])
                        L_top = 0
                        R_top = 0
                        for i in range(len(a)):
                            if L_top == len(L):
                                A[i:] = R[R_top:]
                                break
                            if R_top == len(R):
                                A[i]: = L[L_top:]
                                break
                            if L[L_top] < R[R_top]:
                                A[i] = L[L_top]
                                L_top += 1
                            else: #L[L_top] >= R[R_top]
                                A[i] = R[R_top]
                                R_top += 1
                            
                        return A
                        
                LOOP invariant for merge: at the start of each iteration of the actual 
                    merging,(i) A[p..k-1] is sorted and contains (ii) the k-p smallest 
                    elements of L[1..n1 +1] and R[1..n2 + 1] in sorted order. (iii) L[i] and
                    R[j] are the smallest elements of their subsequences that havent been copied
                    back to A. 
                
                initialization: prior to loop entry, A[p..k-1] is empty, so (i) and (ii) holds. 
                    (iii) holds because L and R are sorted and thus their first elements are 
                    their smallest.
                maintenance: RV? dont need rigor. Go through all possible cases of after each iter
                termination: k = r + 1 at the end. subarray A[p..k-1] = A[p..r+1-1] = A[p..r+1]
                    is sorted and etc
                    
                    MERGESORT(A, p, r)
                        if p < r
                            q = floor( (p + r) / 2 )
                            MERGESORT(A, p, q)
                            MERGESORT(A, q+1, r)
                            MERGE(A, p, q, r)
                    
                    def mergesort(A, p, r):
                        if p != r:
                            q = int( (p + r) / 2)
                            L = mergesort(A[:q+1], p, q)
                            H = mergesort(A[q+1:], q+1, r)
                            return merge(L + H, p, q, r)
                        else:
                            return A
                
                - can write recursive functions as RECURRENCE EQUATIONS or RECURRENCE
                    - describes problem of size n on terms of problem on size less than N
                    
            - RUNNING TIME OF A DIVIDE AND CONQUER ALGORITHM. ****************** RV
                - For an algorithm working on instance of size N, T(N) is the running time. 
                - If N <= c, for some small constant c, then the running time is O(1)
                - Say we divide our instance of size N into A instances/subproblems of size N/B
                    - then it takes T(N/B) to solve one subproblem, so it takes A*T(N/B) to solve all
                - Let D(N) be the time to divide the problem into the subproblems 
                - Let C(N) be the time to combine solutions to the subproblems
                LTX
                if n <= c: T(N) = O(1)
                if n >  c: T(N) = A*T(N/B) + D(N) + C(N)
                
            MERGESORT COMPLEXITY:
                DIVIDE: computing split point takes O(1) time
                CONQUER: splitting array in half and computing solns for both is 2*T(N/2)
                COMBINE: merge is O(N) 
                LTX
                if n <= 1: T(N) = O(1)
                if n >  1: T(N) = 2*T(N/2) + O(1) + O(N)
                    - can use master theorem later to solve recurrence relation so T(N) = O(NLGN)
                    - alternatively we can use intuition (SERIOUSLY?)
                    - or we can draw a RECURSION TREE RV
                        - the recursion tree has log2(n) + 1 levels (derive this.) since we 
                            need to divide N by 2, log2(n) times to get the trivial base case
                            length needed by mergesort, which is 1. Each level costs cn (WHY?)
                            b/c adding up the cost of MERGE on each level besides the bottom yields
                            cn - the bottom is due to having n nodes of size 1 (base case costs c as well)
                            and since we have log2(n) + 1 levels, the complexity is cnlgn + cn
                                - this simplifies to just O(N LG N) since we dgaf about c and cn
                                    when we have N LG N trumping cn in the long run
                                    
                             
    3 Growth of Functions 43
        - ASYMPTOTIC efficiency - large inputs so ORDER OF GROWTH of running time is relevant
        3.1 Asymptotic notation 43
        (need to LATEX LTX all the definitions and theorems in this section)
            - functions describe this have domain in natural numbers, but may be abused to reals
            - Requires that every f(n) in THETA(g(n)) be ASYMPTOTICALLY NONNEGATIVE, aka 
                f(n) be nonnegative whenever n is sufficiently large. ???????????? RV
                - an ASYMPTOTICALLY POSITIVE function is the same, except positive instead nonneg
            - ??? RV g(n) must be asymptotically nonnegative or else THETA(g(n)) empty. 
                - same applies to all other asymptotic notations - all fcns within the notation
                    must be assumed as asymptotically nonnegative
            (1) BIG-THETA-notation
                THETA(g(n)) = {f(n) : there exists constants c1, c2, n0 > 0 s.t. 
                                0 <= c1*g(n) <= f(n) <= c2*g(n)  for all n >= n0}
                - g(n) is an ASYMPTOTICALLY TIGHT BOUND for f(n) and f(n) = THETA(g(n))
                - THETA(g(n)) is a subset of O(g(n)) AKA f(n)=THETA(g(n)) IMPLIES f(n)=O(g(n))
            (2) BIG-O-notation
                O(g(n)) = {f(n) : there exist constants c, n0 > 0 s.t.
                            0 <= f(n) <= c*g(n) for all n >= n0}
                - g(n) is an ASYMPTOTIC UPPER BOUND for f(n) and f(n) = O(g(n))
            - ??? RV O(n^2) bound worst-case insertion sort also applies to its running time ALL input.
                The THETA(n^2) bound worst-case insertion sort, however, does not imply a THETA(N^2)
                bound on the running time of insertion sort on every input.
            (3) BIG-OMEGA-notation
                OMEGA(g(n)) = {f(n) : there exist constants c, n0 > 0 s.t.
                                0 <= c*g(n) <= f(n) for all n >= n0}
                - g(n) is an ASYMPTOTIC LOWER BOUND for f(n) and f(n) = OMEGA(g(n))
                
            THEOREM: for two functions f(n) and g(n), we have that
                f(n) = THETA(g(n)) <===> f(n) = O(g(n)) AND f(n) = OMEGA(g(n))
                
        RV REVISIT ????????????????????????????????????????        
"The running time of insertion sort therefore belongs to both OMEGA(N) and O(N),
since it falls anywhere between a linear function of n and a quadratic function of n.
Moreover, these bounds are asymptotically as tight as possible: for instance, the
running time of insertion sort is not OMEGA(N^2), since there exists an input for which
insertion sort runs in THETA(N) time (e.g., when the input is already sorted). It is not
contradictory, however, to say that the worst-case running time of insertion sort
is OMEGA(N^2), since there exists an input that causes the algorithm to take OMEGA(N^2) time."

            - can use asymptotic notation to kill off alot of clutter and make conciseness

            - non-tight notation doesnt give tight bound, and holds for all consts c instead of just
                having need at least one const c exist
            (4) little-o notation
                o(g(n)) = {f(n) : for all const c >= 0, exists const n0 > 0 s.t.
                            0 <= f(n) < c*g(n) for all n >= n0}
                - f(n) is ASYMPTOTICALLY SMALLER (& not tight) than g(n) AND f(n) = o(g(n))
                - lim n->INF of (f(n) / g(n)) = 0
            (5) little-omega notation
                omega(g(n)) = {f(n) : for all const c >= 0, exists const n0 > 0 s.t.
                            0 <= c*g(n) < f(n) for all n >= n0}
                - f(n) is ASYMPTOTICALLY LARGER (& not tight) than g(n) AND f(n) = omega(g(n))
                - lim n->INF of (f(n) / g(n)) = INF
                
            - asymptotic functions follow TRANSITIVITY, REFLEXIVITY, SYMMETRY, TRANSPOSE SYMMETRY,
                just like how real numbers do. Strong analogy b/w 5 notations and =, >, <, <=, >=
                - however, TRICHOTOMY doesn't hold because not all fcns asymptotically comparable. 
            
        3.2 Standard notations and common functions 53
            - monotonic/strictly increasing/decreasing, floor/ceil, modulo, polynomial asymptotically
                positive, polynomially bounded, exponentials, e, logarithms, polylogarithmically 
                bounded, factorials/stirling's approx, functional iteration, iterated log fcn, 
                fibonacci + golden ratio
                
    4 Divide-and-Conquer 65
        (1) DIVIDE the problem into subproblems that are smaller instances of the
            same problem
        (2) CONQUER the subproblems by solving them recursively. (aka, divide and
            conquer them as well). If small enough, solve them straightforward 
        (3) COMBINE solutions to all subproblems into solution to original problem
        - the RECURSIVE CASE is when problems are big enough to solve recursively/conquer
        - the BASE CASE is when recursion bottoms out and subproblems are small enough to be solved fast
            - subproblems actually not actually be all the same format
        - a RECURRENCE is an eqn/inequality defining a function in terms of smaller inputs
        - a recurrence inequality we solve to get OMEGA(N) or O(N) (instead of eqn where we get THETA(N))
        - assumptions: USUALLY ignore floors/ceil & bdry conditions (for small/const input size)
        
        4.1 The maximum-subarray problem 68
            PROBLEM: same as EPI intro problem. Given an array where the ith entry is the stock 
            price on day i (thus the days are sequential w.r.t. the indices), find the maximum 
            possible margin (ie find the max difference in the array s.t. the smaller value occurs
            before the larger value)
            
            - brute force soln - compute for all pairs of days = n choose 2 = theta(N^2)
            
            (my epi solution)
            def max_margin(A):
                current_smallest_element = A[0]
                Max = float("-Inf")
                for item in A:
                    if item > current_smallest_element:
                        if item - current_smallest_element > Max:
                            Max = item - current_smallest_element
                        else:
                            current_smallest_element = item
                return Max
            
            - CONVERT this problem into a max-subarray problem by generating an array of
            length len(A) - 1, where the i-th index of this new array is the sum of A[i] and
            A[i-1]
            
            Divide and conquer algorithm:
            - still followed paradigm of DIVIDE, CONQUER, COMBINE. 
            - want to divide into two N/2 sized subarrays, recursing. 
            - base case is when N = 1, max subarray at N=1 is itself
            
            - the max subarray, after DIVIDE, then must be inside either
                (1) the left subarray (from index 0 to mid)
                (2) the right subarray (from index mid+1 to len(arr)-1)
                (3) must cross the middle. 
                - (1) and (2) follow divide and conquer steps, so its easier
                - (3) does not, so it must be in the combine step, and needs thinking
            - the max crossing subarray MUST consist of two subarrays whose indices 
                end at mid for the left subarray, and begin at mid+1 for the right subarray
                - the max crossing subarray then is the concat of those two outputs
                - checking all subarrays on left subarray ending at M costs N/2, 
                    and same with on right, so it is a O(N) operation.
            
            FIND-MAX-CROSSING-SUBARRAY(A, low, mid, high)
                left_sum = -Inf
                max_left = mid
                sum = 0
                for i = mid to low
                    sum = sum + A[i]
                    if sum > left_sum
                        left_sum = sum
                        max_left = i
                right_sum = -Inf
                max_right = mid + 1
                sum = 0
                for j = mid + 1 to high
                    sum = sum + A[j]
                    if sum > right_sum
                        right_sum = sum
                        max_right = j
                return (max_left, max_right, left_sum + right_sum)
            
            - Now we build the recursive framework
            
            FIND-MAX-SUBARRAY(A, low, high)
                if low == high
                    return (low, high, A[low])
                mid = (high + low + 1) / 2
                left_low, left_high, left_sum = FIND-MAX-SUBARRAY(A, low, mid)
                right_low, right_high, right_sum = FIND-MAX-SUBARRAY(A, mid+1, high)
                cross_low, cross_high, cross_sum = FIND-MAX-CROSSING-SUBARRAY(A, low, mid, high)
                if left_sum >= right_sum and left_sum >= cross_sum
                    return left_low, left_high, left_sum
                else if right_sum >= left_sum and right_sum >= cross_sum
                    return right_low, right_high, right_sum
                else
                    return cross_low, cross_high, cross_sum
                    
            FIND-MAX-SUBARRAY complexity
                - Each line costs O(1), except for FIND-MAX-SUBARRAY, which is T(N/2)
                    and FIND-MAX-CROSSING-SUBARRAY, which is O(N). 
                    - analysis is SAME as mergesort. read that one
            RV
            Kadane's algorithm (also linear)
            def max_subarray(A):
                max_ending_here = max_so_far = A[0]
                for x in A[1:]:
                    max_ending_here = max(x, max_ending_here + x)
                    max_so_far = max(max_so_far, max_ending_here)
                return max_so_far
                    
            def max_subarray_linear(A):
                Max_global = float("-Inf")
                Low_global = High_global = None
                Max_local = 0
                Low_local = 0
                for i in range(len(A)):
                    Max_local += A[i]
                    if Max_local > Max_global:
                        Low_global = Low_local
                        High_global = i
                        Max_global = Max_local
                    if Max_local < 0:
                        Max_local = 0
                        Low_local = i + 1
                return (Low_global, High_global, Max_global)  
            
        
        4.2 Strassenâ€™s algorithm for matrix multiplication 75
            
            SQUARE-MATRIX-MULTIPLY(A,B)
                N = A.rows
                C = new NxN matrix
                for i = 1 to N
                    for j = 1 to N                        C[i][j] = 0                        for k = 1 to N                            C[i][j] = C[i][j] + A[i][k] * B[k][j]                return C
            - COMPLEXITY: THETA(N^3)                    - DIVIDE AND CONQUER for matrix product: divide matrix into 4 n/2 x n/2 matrices                        SQUARE-MATRIX-MULTIPLY-RECURSIVE(A, B)                N = A.rows                C = new NxN matrix                if N == 1                    C[1][1] = A[1][1] * B[1][1]                else                    M = (N / 2)                     one = 1:M                    two = M+1:N-1                    C[one][one] =                         SQUARE-MATRIX-MULTIPLY-RECURSIVE(A[one][one], B[one][one]) +                        SQUARE-MATRIX-MULTIPLY-RECURSIVE(A[one][two], B[two][one])                    C[one][two] =                         SQUARE-MATRIX-MULTIPLY-RECURSIVE(A[one][one], B[one][two]) +                        SQUARE-MATRIX-MULTIPLY-RECURSIVE(A[one][two], B[two][two])                    C[two][one] =                         SQUARE-MATRIX-MULTIPLY-RECURSIVE(A[two][one], B[one][one]) +                         SQUARE-MATRIX-MULTIPLY-RECURSIVE(A[two][two], B[two][one])                    C[two][two] =                        SQUARE-MATRIX-MULTIPLY-RECURSIVE(A[two][one], B[one][two]) +                        SQUARE-MATRIX-MULTIPLY-RECURSIVE(A[two][two], B[two][two])                                return C            - cost of divide is O(1) b/c index slicing            - cost of each call is T(N/2) since we define T(N) to be the cost of multiplying            two NxN matrices and each call is multiplying two N/2 X N/2 matrices. 8 calls            - cost of combine is O(N^2) b/c each addition is going through a submatrix            that is N/2 x N/2, therefore has N^2/4 elements, and there's 4 submatrices,             so 4 * n^2/4 is N^2. (for the latter could just say O notation drops 1/4)            T(N) = a * T(N/b) + D(N) + C(N)            T(N) = 8 * T(N/2) + O(1) + O(N^2)  . Use master theorem to solve and get            - strassen's method                (1) divide A & B & output C into 4 n/2xn/2 matrices like ^^^. Cost O(1)                (2) create 10 matrices all n/2xn/2 that are sums/diffs of (1). O(N^2)                (3) using (1) and (2), recursively compute 7 products of n/2 x n/2                (4) compute C11 C12 C21 C22 of result by add/subtract in (3). O(N^2)                T(N) = 7*T(N/2) + O(N^2)
        4.3 The substitution method for solving recurrences 83            (1) Guess the form of the solution            (2) Use PMI to find constants and show solution works
        4.4 The recursion-tree method for solving recurrences 88            - each node of the tree represents the cost of a single subproblem.            - GETTING DEPTH.                - if the algorithm taking N inputs splits the problem into subproblems of size N/B,                    then the subproblem size for a node at depth i is (N/(B^i))                    - there are A^i nodes at depth i (kinda pointless for this part)                    - the depth when subproblem size is 1 is then 1 = (N/(B^i))                         => i = LOG BASE B OF N                         - change 1 to M = minimum size for base case and we get                        M = (N /(B^i)) => B^i = N/M => i = LOG base B of (N/M)                            - i is the height of the recursion tree             - PER LEVEL COST.                - the algorithm splits N inputs into N/B subinputs for A subproblems                - let f(n) be the cost for D(N) and C(N) combined                - the cost at level i is then (A^i) * f(N/(B^i))                    - the cost of one node at level i is then just f(N/(B^i))                    - there are A^i nodes at level i                - the cost at the ground/leaf level is A^(LOG base B of (N/M))                    - there are that many nodes at the bottom                    - A^(LOG base B of N) == N^(LOG BASE B OF A) == cost at bottom            - the cost for the entire tree is then                T(N) = A^(LOG base B of (N/M))                     + SUM from (i=0 to LOG base B of (N/M) - 1) OF [(A^i) * f(N/(B^i))]                            - assuming that the cost at base case is 1            - MISC: the number of nodes in a tree follows a geometric series -.<                        - when given subinput splitting s.t. subinputs are not equal, follow the                 tree branch where the subinput splitting retains the most element                    - ex: T(n) = T(n/3) + T(2n/3) + O(n)                        - B IS NOT 2/3 here, its 3/2!!!!!!!!                        - follow T(2n/3) down to m (which is the base case, probably 1)                            - depth (when base case =1) = LOG base 3/2 of N                        - use the equations above.                        - its messy though, but intuitively we expect the complexity to be                            on the order of the number of levels times the cost at each level                        - (log base 3/2 of N + 1) levels * (n) cost of combine+divide each level                            - n * log base 3/2 of n = n log n
        4.5 The master method for solving recurrences 93            - if we have a recurrence of the form                (1) T(N) = a*T(n/b) + f(n)                (2) a >= 1 & b > 1 are BOTH constants, and                 (3) f(n) is an asymptotically positive function                     - we can quickly solve this recurrence using the master method.                        MASTER THEOREM:            Let a >= 1 and b > 1 be constants. Let f(n) be a function. Let T(n) be defined            on the nonnegative integers by the recurrence                T(n) = a*T(n/b) + f(n)            where we interpret n/b to mean either floor(n/b) or ceil(n/b). Then, T(n)            has the following asymptotic bounds (E is epsilon here):                (1) If f(n) = O(n ^ log base b of (a - E)) for some constant E > 0,                    then T(n) = THETA(n ^ log base b of a)                (2) If f(n) = THETA(n ^ (log base b of a)),                     then T(n) = THETA( (n ^ (log base b of a)) * log n)                (3) if f(n) = OMEGA(n ^ log base b of (a + E)) for some constant E > 0,                    AND if a*f(n/b) <= c*f(n) for some constant c < 1 and all large n,                    then T(n) = THETA(f(n))                        - BASIC IDEA:             (i) evaluate LOG BASE B OF A.                (ii) compare with the POWER of f(n)            (iii) pick (1) (2) or (3) depending on the comparison                - under (1) and (3), (log base b of a) has to be POLYNOMIALLY greater or                    less than power(f(n)) to use master theorem                    - for example, n^(log base b of a) = n^1 <= n lg n, but is not                         polynomially larger, so we can't apply (3) here.                - NOTE: there is a gap between 2 and 3 and 1 and 2 such that ^^ applies            - the master theorem is super important. IT reads it as says.                
        4.6 Proof of the master theorem 97
    5 Probabilistic Analysis and Randomized Algorithms 114
        5.1 The hiring problem 114            - I get a hire rec from agency every day. Committed to having best always.                 costs money to fire and hire and pay agency. want to estimate cost.                        HIRE-ASSISTANT(N)                best = 0                 for i = 1 to N                    interview candidate i                    if candidate i is better than candidate best                        best = i                        hire i            complexity: O(c_i * n + c_h * m). m = # hired. c_i = cost interview. c_h = cost hire                    - the AVERAGE CASE RUNNING TIME is the running time of an algorithm averaged                over the distribution of all possible inputs                - you might want to assign a weight to each possible input =)            - a UNIFORM RANDOM PERMUTATION is one where any of the possible permutations                occur with equal probability            - a RANDOMIZED algorithm is random if inputted is via RNG or input values are random            - a PSUEDORANDOM NG is deterministic but the outputs look random                - EXPECTED RUNNING TIME is the running time of a randomized algorithm                    - not the same as algorithms where just the input is random. Those are nomies
        5.2 Indicator random variables 118            - basically is a random variable that allows terms in a sequence to occur or not occur                - takes on values of 1 and 0 only - obv if 1, its producted term occurs, otherwise doesnt            LEMMA: Given a sample space S and an event A in the sample space S, let X_A = I{A}.                Then, E[X_A] = Pr{A}
        5.3 Randomized algorithms 122
        5.4 Probabilistic analysis and further uses of indicator random variables 130

II Sorting and Order Statistics
    Introduction 147        - a RECORD is a collection of data, whose elements are KEYS used for sorting. SATELLITE            DATA is data that comes along with the keys.         - a sorting algorithm sorts IN PLACE if a const num of elements of input array are stored
            outside the array. Any more and it is no longer considered in place        - INSERTION SORT is good in-place for small inputs since its nested loop is tight,             even though it runs in O(N^2)         - MERGESORT is faster (O NLGN) but uses O(N) space in MERGE/COMBINE step.        - HEAPSORT sorts in place in NLGN using a heap        - QUICKSORT sorts in place in N^2 worst-case, but actually avgs at NLGN. Good for LARGE sets            - above is all COMPARISON SORTS - determine order by comparing elements.                 - there is a proof that the fastest possible comparison sort must be NLGN        - COUNTING SORT, RADIX SORT, BUCKET SORT require assumptions on the dataset, but can             operate in O(N) linear time         Let T = THETA, and O be Big-O                                WORST-CASE              AVERAGE/EXPECTED        Insertion sort          T(N^2)                  T(N^2)        Merge sort              T(N LG N)               T(N LG N)        Heapsort                O(N LG N)               not analyzed in this book        Quicksort               T(N^2)                  T(N LG N)        Counting sort           T(k + n)                T(k + n)        Radix sort              T(d(n + k))             T(d(n + k))        Bucket sort             T(N^2)                  T(N)
    6 Heapsort 151        - in-place like insertion sort (doesnt waste space)! NLGN like mergesort (FAST!!!)        - uses a MAX heap data structure (not a min heap!)
        6.1 Heaps 151            - binary tree complete except maybe lowest level            - representable as array. convention has length be total length of array, and heap-size                to be the length of the array actually used by the current heap 0<=heapsize<=len            - heap root at A[1] (first element of array)            - given a node i, (aka node in heap as eleemnt in array                PARENT node is floor(i/2)   - implemented as right-shift                LEFT CHILD node is 2i       - implemented as left-shift                RIGHT CHILD node is 2i + 1  - implemented as left-shift + 1            - MIN HEAPS have smallest value at root, MAX HEAPS have largest value at root            - HEAP PROPERTY is that for every node besides the root, the parent of this node                 must be <= or >= this node, analogous to MIN-HEAP-PROP & MAX-HEAP-PROP resp.            - the HEIGHT of a node in the heap is the longest downward path from this node to a leaf                - HEIGHT OF A TREE is just the height of the root = THETA(LG N)                        MAX-HEAPIFY O(LG N) - maintains max-heap property            BUILD-MAX-HEAP O(N) - generates max-heap from random array.            HEAPSORT O(N LG N)  - sorts an array in place                        MAX-HEAP-INSERT, HEAP-EXTRACT-MAX, HEAP-INCREASE-KEY, HEAP-MAX - O(LG N)                - help make a heap a priority queue            LEFT(i) { return 2i }            RIGHT(i) { return 2i + 1 }            PARENT(i) { return floor(i/2) }
        6.2 Maintaining the heap property 154            - takes in an array (a heap!) and an index. questions if the node at this                 index satisfies the heap property. Compares it, its left kid, and right kid                altogether. The largest value of the three is swapped with its position (in                 line with max heap property) and we recursively call it again to make sure                the value swapped also satisfies heap property with its new children. LG N b/c                 can only make LG N recursive calls since that is the height of whole tree.                        MAX-HEAPIFY(A, i)                l = LEFT(i)                r = RIGHT(i)                if l <= A.heap_size and A[l] > A[i]                    largest_index = l                else                    largest_index = i                if r <= A.heap_size and A[r] > A[largest_index]                    largest_index = r                if largest_index != i                    temp = A[largest_index]                    A[largest_index] = A[i]                    A[i] = temp                    MAX-HEAPIFY(A, largest_index) #no longer largest and in question.                        Complexity:                T(N) <= T(2n/3) + O(1) -----> T(N) <= O(n^ log base 3/2 of 1 * LG N) = O(LG N). master thmhttps://math.stackexchange.com/questions/181022/worst-case-analysis-of-max-heapify-procedure                OR, T(N) for MAX-HEAPIFY on a node of height H is O(H) 
        6.3 Building a heap 156            - convert any array to a max-heap via calling MAX-HEAPIFY. All nodes to the right of the middle            are already leaves, so we dont need to call MAX-HEAPIFY on them. Start at the middle of the array,            and call MAX-HEAPIFY on every index from the middle to the beginning iteratively, (which makes             recursive calls).             BUILD-MAX-HEAP(A)                A.heap-size = A.length                for i = floor(A.length/2) downto 1                    MAX_HEAPIFY(A, i)            - the LOOP INVARIANT of BUILD-MAX-HEAP is that for the loop at (1) INIT, (2) MAINT, (3) TERM,            every index to the right of the current index is the root to a max-heap.            - UPPER BOUND: MAX_HEAPIFY costs O(LG N) at worst case, and since we iterate through             about N/2 calls to MAX-HEAPIFY, it costs O(N LG N)            - TIGHTER BOUND: There are N / 2^(H + 1) nodes at height H of the heap. Each node at             height H calling MAX_HEAPIFY costs O(H) to call it (since the complexity of MAX_HEAPIFY            is O(LG N) for a heap with N nodes - LG N is the highest height of said heap at root).            Since H ranges from 0 (at leaves) to LG N (at root), the total cost is then            T(N) = SUM from (h=0 to LG N) OF [ (N/2^(H+1)) * O(H)] = O(N), via geometric series            - minheap works same way
        6.4 The heapsort algorithm 159            - take an array, BUILD-MAX-HEAP from it in O(N) time. 1st element is then maximum val.            swap that with the last value (aka put the biggest element at the end). decrement the 'size'            of the heap, so we don't notice the last element of the array as being part of the heap anymore.            Call MAX-HEAPIFY on the new swapped root to get to get a new max-heap. do for all elements of arr                    HEAPSORT(A)                BUILD-MAX-HEAP(A)                for i = A.length down to 2                     exchange A[1] with A[i]                    A.heap-size = A.heap-size - 1                    MAX-HEAPIFY(A, 1)            COMPLEXITY: BUILD-MAX-HEAP costs O(N) + MAX_HEAPIFY costs O(LG N), done N times, so complexity is                 O( N LG N )
        6.5 Priority queues 162            - PRIORITY QUEUES manage a set A of elements, each with associated KEY value. both MAX and MIN                priority queues exist. We detail MAX-PRIORITY QUEUE (the min is just opposite everything)                                    HEAP-MAXIMUM(A)             - O(1). returns element of A with largest key                        return A[1]                    HEAP-EXTRACT-MAX(A)         - O(LG N) removes and returns element of A with largest key                        if A.heap-size < 1          - all things const + MAX_HEAPIFY = LG N                            error "heap underflow"      - similar to heapsort                        max = A[1]                        A[1] = A[A.heap-size]                        MAX-HEAPIFY(A, 1)                        return max                    HEAP-INCREASE-KEY(A, i, key)  - O(LG N) change val of element at i to key (need be bigger)                        if key < A[i]               - b/c worst case i is a leaf and need go up LG N = height                            error "new key smaller than current key"                        A[i] = key                        while i > 1 and A[PARENT(i)] < A[i]                            exchange A[i] with A[PARENT(i)]                            i = PARENT(i)                    MAX-HEAP-INSERT(A, key)             - O(LG N) inserts key into A                        A.heap-size = A.heap-size + 1       - b/c HEAP-INCREASE-KEY is O LG N                        A[A.heap-size] = -Inf                        HEAP-INCREASE-KEY(A, A.heap-size, key)
    7 Quicksort 170        - worst case N^2, but average N LG N. best practical. const fact in N LG N are small. sorts in place
        7.1 Description of quicksort 170            - divide and conquer sorting array A[p..r]                (1) DIVIDE: partition A[p..r] into two possible empty subarrays A[p..q-1] and A[q+1..r] such that                    x <= A[q] <= y for all x in A[p..q-1] and for all y in A[q+1..r]. Compute q.                 (2) CONQUER: sort subarrays A[p..q-1] and A[q+1..r] via recurisve quicksort calls                (3) COMBINE: subarrays are sorted, so no work to combine. A[p..r] sorted.                QUICKSORT(A, p, r)                    if p < r                        q = PARTITION(A, p, r)                        QUICKSORT(A, p, q-1)                        QUICKSORT(A, q+1, r)                PARTITION(A, p, r)         - this is the key to the algorithm, rearranging in place.                    x = A[r]                    i = p - 1                    for j = p to r - 1                        if A[j] <= x                            i = i + 1                            exchange A[i] with A[j]                     exchange A[i+1] with A[r]                    return i + 1                - LOOP INVARIANT: at each iteration of the loop,                     (1) every item to the left of i is <= the pivot                    (2) every item to the right of i up til the current jth iteration index is > pivot                    (3) the item where the pivot is is equal to the pivot                - PARTITION picks the last element of the array as the 'pivot element'. There is a                counter i for 'where the pivot element is going to be at the end of the fcn'. We iterate                through each index j of the array up until the last (which is the pivot) element, comparing                the pivot to each jth element, and if it is <= the pivot element, we increment i and swap                 the j-th element with the i-th element. Essentially, we're moving all elements less than                 the pivot before i and leaving all elements greater than pivot after i. At the end of the loop,                we swap the pivot element with, i+1, putting it in its correct place. Now everything                 right of the new pivot position is greater, and everything left is less than or equal to pivot.                    - partition runs in O(N)
        7.2 Performance of quicksort 174            - in worst case scenario, partitions into N-1 and 0 elements, N times.                 T(N) = T(N - 1) + T(0) + O(N) = T(N - 1) + O(N)            - will occur N times, and each time is N cost for partition, so complexity is N^2            - in best case scenario, partitions into ~N/2 and N/2 elements                T(N) = 2 * T(N/2) + O(N)            - will occur LG N times since you need to split N elements in half LG N times to get to                 the base case. Then complexity is N LG N            - in the average scenario, even if its a TERRIBLE case, such as N/10 and 9N/10 elements,                T(N) = T(9N/10) + T(N/10) + O(N)            - if this occurs at every node, we follow the longest branch, which is all T(9N/10).                 - the depth of this branch (which is that of the tree) is LOG BASE (10/9) of N,                    which is on order of O(LG N).                     - Therefore, interestingly enough, even in this 'shitty' case, complexity is NLGN.                    happens to be for all splits of CONSTANT proportionality, still converges to LG N height,                    which gives us N LG N complexity after the height has be O-notation-ized.
        7.3 A randomized version of quicksort 179            - basically same as quicksort, but randomly pick the pivot at each step instead of using the                last element of the (sub)array as the pivot. 
        7.4 Analysis of quicksort 180            - proof of worst case quicksort is N^2            - im bitching out. too much math
    8 Sorting in Linear Time 191
        8.1 Lower bounds for sorting 191            - comparison sorts cannot get any better than N LG N
        8.2 Counting sort 194            - no comparisons are done in counting sort. the actual index values are used to do things                        - CONDITIONS: each of the N input elements in A is an integer in range 0 to k            COUNTING-SORT(A,k)                // B will be the returned sorted array filled in later                B = new array A[1..A.length]                // C initially will contain counts for occurrences of each integer in A. strts @ 0                C = new array C[0..k]                for i = 0 to k                    C[i] = 0                // populate C with the counts of occurrences for each integer in A                for j = 1 to A.length                    C[A[j]] += 1                // each index in C will soon contain the counts of all the integers <= to tht index                for i = 2 to k                    C[i] += C[i-1]                // now go through each element of A, find counts, put it where it belongs in B                for j = A.length downto 1                    B[C[A[j]]] = A[j]                    C[A[j]] = C[A[j]] - 1            - runs in O(N)            - a STABLE sorting algorithm sorts array elements s.t. numbers with the same value appear            in the output array in the same order as they do in the input array. 
        8.3 Radix sort 197            ????????????????????????????????????
        8.4 Bucket sort 200
    9 Medians and Order Statistics 213
        9.1 Minimum and maximum 214
        9.2 Selection in expected linear time 215
        9.3 Selection in worst-case linear time 220

III Data Structures    Introduction 229    - sets in CS are FINITE and DYNAMIC    - a dynamic set supporting insertion, deietion, and membership ops is a DICTIONARY    - sets contain objects, which contains KEYS and SATELLITE DATA    - sometimes objects in a set are ordered in some way    OPERATIONS:         SEARCH(S, k): given set S and key k, return pointer x to element in S with key k        INSERT(S, x): modifies element pointed to by x in S in some way        DELETE(S, x): deletes element pointed to by x in S        MINIMUM(S): returns pointer x to element in S with smallest key        MAXIMUM(S): returns pointer x to element in S with biggest key        SUCCESSOR(S, x): in ordered S, returns element with key next greater than x in S        PREDECESSOR(S, x): in ordered S, returns element with key next less than x in S

    10 Elementary Data Structures 232
        10.1 Stacks and queues 232 - ALWAYS O(1) insert/delete, O(N) access/search, O(N) space            - STACK: LIFO.                 - S[1..N]. S.top. Stack is S[1..S.top]                - S.top == 0  implies stack empty. POP empty stack == underflow                - S.top = N implies stack full. PUSH full stack == overflow                - PUSH == INSERT                 - POP == DELETE                STACK-empty(S)                    if S.top == 0                        return True                    else                        return False                PUSH(S, x)                    if S.top == S.length                        error 'overflow'                    S.top = S.top + 1                    S[S.top] = x                POP(S)                    if STACK-EMPTY(S)                        error 'underflow'                    else                         S.top = S.top - 1                        return S[S.top + 1]                            - QUEUE: FIFO. (circular)                - Q[1..N]. Q.head. Q.tail                    - head and tail start at index 1 for both                    - tail points to current FREE entry                    - head points to                 - EMPTY when Q.head == Q.tail. dequeue empty = underflow                - FULL when Q.head = Q.tail + 1. enqueue full = overflow                - INSERT == ENQUEUE                - DELETE == DEQUEUE                ENQUEUE(Q, x)                    Q[Q.tail] = x                    if Q.tail == Q.length                        Q.tail = 1                    else                        Q.tail = Q.tail + 1                                    DEQUEUE(Q)                    x = Q[Q.head]                    if Q.head == Q.length                        Q.head = 1                    else                        Q.head = Q.head + 1                    return x                            
        10.2 Linked lists 236  -  O(1) insert/delete, O(N) access/search/space. no space limit.                - NOTE that insertion and deletion doesnt take into account searching for item to                    delete or searching if item is already in list before insert.                     - talking about relatively to an array, cuz when u delete/insert in array, you                        shift all elements to the side in O(N) cost.            - each element is node with attribute key and ptr next and prev            - DOUBLY LINKED LISTS - has prev and next ptrs            - SINGLY LINKED LISTS - has only next ptrs            - CIRCULAR LINKLED LISTS - tail node next pts to head node. if not circular tail next =null            - DUMMY NODE (not in book) - have a node not part of list pt to head node.            LIST-SEARCH(L, k)                // init 'local' node to be head                x = L.head                // go through each node (until its null) unless we find the node with the key                 while x != NULL and x.key != k                    x = x.next                // return the node with the key                return x                        LIST-INSERT(L, x)   //for doubly linked. insert at HEAD (puts in front)                // connect old head to new head, which is x                x.next = L.head                 if L.head != NULL                    L.head.prev = x                // set the lists new headptr to x                L.head = x                // for doubly linked set prev ptr to null                x.prev = NULL            LIST-DELETE(L, x) // x is alrady given as a connected node inside L                if x.prev != NULL                    x.prev.next = x.next                else                    L.head = x.next                if x.next != NULL                    x.next.prev = x.prev            - SENTINEL - node of same type of other nodes in list, but represents NULL
                - can turn a regular doubly linked list to a circular, doubly linked list with sentinel
                
        10.3 Implementing pointers and objects 241
        10.4 Representing rooted trees 246            - BINARY TREE: nodes x with members x.left and x.right. if = null, the node doesnt exist in tree            - K-KID TREE: node x with k-sized list/arr of nodes x. doesn't work when k unbounded.            - LEFT-CHILD, RIGHT SIBLING REPRESENTATION: have x.left point to leftmost child of x, then have                x.right point to the right sibling of x, and x.parent point to the parent of x. rightmost siblings                 have x.right point to null. parents with no kids have x.left = null
    11 Hash Tables 253  - O(1) search, access, delete        - ONLY INSERT, SEARCH, DELETE all O(1) average, O(N) worst for bad hash function         - key is sent through a hash function which yields a unique index in the array this unique            key will be at            - bad hash functions have frequent collisions between hashes for the indexes incoming                 keys will be at; in this case, either the next available array entry will be the                 index for the incoming key, or the same index now contains a CHAIN between two                keys, where the incoming key will be at the end of the chain            - hash tables use more space than needed to achieve good hashing 
        11.1 Direct-address tables 254            - DIRECT ADDRESS TABLE length of universe U of possible keys == size of array holding hash table
                - each SLOT of the table corresponds uniquely to a key in universe                - EXACTLY like how counting sort works. Slot k in table contains an element with key k                    - therefore we need a table T of size == MAXIMUM value in universe of possible keys                        - obviously a waste of space if large gap b/w possible key values.                             - example: U = (1,100). U_used =[1, 2, 100]. then T[1..100]. only use 3/100 slots =)                DIRECT-ADDRESS-SEARCH(T, k)                    return T[k]                DIRECT-ADDRESS-INSERT(T, x)                    return T[x.key] = x                            DIRECT-ADDRESS-DELETE(T, x)                    T[x.key] = null            
        11.2 Hash tables 256            - let size(T) <<< size(U) b/c size(set K) of possible keys <<<< size(U).             - a key of value k is then stored at slot H(k) in T, where H is a hash function converting                from elements in the set U to slot sizes in the slot set of T.             - if two distinct keys k1, k2 have H(k1) == H(k2), then we have a collision                - ideally fix by having a better hash function (cant solvee eeverything.)                - chaining would put k1 and k2 in the same slot, but there would be pointers to the next                    key that is going to collide into this spot like a linked list                                CHAINED-HASH-INSERT(T, x)       - always O(1) unless checking if x is already there                        insert x at the head of list T[H(x.key)]                                        CHAINED-HASH-SEARCH(T, X)       - highest complexity. O(len(T[h(k)])). Other two methods need this                         search for an element with key k in list T[h(k)]                            CHAINED-HASH-DELETE(T, x)       - O(1) ALONE, but u gotta search for the item first before delete.                        delete x from list T[h(x.key)]                - LOAD FACTOR, ALPHA or A, for a hash table T with M slots storing N elements is N/M is the avg # of elements stored                 in a chain                 - SIMPLE UNIFORM HASHING: any element is equally likely to be hashed into any of M slots independent of                     other elements being hashed.                - WORST CASE: all N keys hash to same slot, making list length N. same as Linkeed list                - AVERAGE CASE: assume simple uniform hashing. Then search takes O(1 + a) time                    - if  N is proportional to M, then N = O(M), so alpha = n/m = O(M)/M = O(1)                        - so searching takes O(1) on average. 
        11.3 Hash functions 262            - hash functions should                 (1) satisfy SIMPLE UNIFORM HASHING                     - kind of hard to check this since you dont know probability distribution of keys.                    - if you DO know distribution, can model it.                        - EX: k is uniformly distributed over 0 <= k < 1                (2) be deterministic                (3) hard to get collisions (might be part of 1)                (4) quick to compute                (i) yields hash independent of patterns in data             - most hash functions interpret keys as being elements of the set of natural numbers            - division hash method: m is # of slots in hash table.                H(k)= k mod m.                     - avoid certain values for m, such as powers of 2                    - prime not too close to exact power of 2 is good choice for m            - multiplication hash method: let A be a const s.t. 0 < A < 1                H(k) = floor(m * (kA mod 1)) == floor( m * (kA - floor(kA)))                    - the value of m is not critical here. usually choose to be power of 2                    - A restricted to be fraction of form s/(2^w), s.t. 0 < s < 2^w,                        - w is word size of machine s.t. k fits into single word            - UNIVERSAL HASHING: choose hash function randomly independent of keys good performance average                - anybody can choose keys based of the hash function to all collide. If random, they cant.                - randomly select hash function from a designed class of hash functions at hash step                    - on average, low probability that two randomly chosen hash functions will hash two                        different keys to the same index                - let H be finite collection of hash functions that map a universe of keys U into range                    {0..m-1}. H is UNIVERSAL if for all pairs of keys k1, k2 in U, the number of hash functions h in H                    such that h(k1) == h(k2) is at most |H| / m.                 - THEOREM: assume hash function h randomly chosen from universal H used to hash n keys into                    table T of size m, using chaining                    (1) if key k not in table, expected length of list occurring at the index in T where k hashes to                        is the load factor alpha = n / m                    (2) if key k is in table, expected length is at most 1 + alpha                - COROLLARY - using universal hashing and chaining on an empty table T with m slots, it takes                     expected O(N) time to handle any sequence of n insert/search/delete ops containing O(m) insert ops                - NUMBER THEORY: an example designed class of universal hash functions is                    H_pm = {h_ab : a is an element of Z*_p and b is an element of Z_p}, where                    h_ab(k) = ((ak + b) mod p) mod m, where                        b is an element of Z_p, Z_p = {0, 1, .., p-1}                        a is an element of Z*_p, Z*_p = {1, 2, .., p - 1}                        p is a prime number, m is number of slots in table, and p > m
        11.4 Open addressing 269            - OPEN ADDRESSING is essentially a hash table without chaining, so when collisions occur, they are probed                to another entry in a hash table. Sequence for probe depends on another hashing of the key                - LINEAR PROBING - has you look at just the next element, and then the next element, etc when collision occur. delta=1.                - QUADRATIC PROBING - has you the probe interval increase linearly. delta = O(n)                - DOUBLE HASHING - probe interval determined by another hash fcn hashing the key again. delta = H_p(k)
            - UNIFORM HASHING: probe sequence of each key equally likely to be any of the m! permutations of (0,..,m-1)            - FAIL SEARCH: given open address hash table with load factor = alpha = n/m < 1, the expected number of probes in                 an unsuccessful search is at most 1/(1-alpha), assuming uniform hashing            - GOOD SEARCH: successful search of ^^^ with load factor alpha < 1, expected probes are given uniform hashing                (1/alpha) * ln (1/1-alpha)            - INSERT: inserting element into ^^^ with load factor alpha requires at most 1/(1-alpha) probes on avg w uniform hashing        11.5 Perfect hashing 277            - a hash is PERFECT if O(1) memory accesses are required to search in the worst case
    12 Binary Search Trees 286        - can be used as dict or priority queue. SEARCH, MIN, MAX, PREDECESSOR, SUCESSOR, INSERT, DELETE
        - ops proportional to tree height. complete binary tree N nodes H = LG N. If linear chain though, then H = N        12.1 What is a binary search tree? 286            - its a binary tree with nodes having left, right, parent, and key s.t. for each node,                 left.key <= key, right.key >= key, the root node has root.parent = null, and leaves have left == right == NULL
            - BINARY-SEARCH-TREE-PROPERTY (same as above!!): let x be a node in a BST. if y is a node in the left subtree of x,                then y.key <= x.key. If y is a node in the right subtree of x, then y.key >= x.key                        - traversal is O(N) cuz there are N nodes in a tree.            INORDER-TREE-WALK(X): //goes through and prints out keys in the tree in order                if x != null                    INORDER-TREE-WALK(X.left)                    print X.key                    INORDER-TREE-WALK(X.right)            PREORDER-TREE-WALK:  //prints root before subtree nodes                if x != null                    print X.key                    PREORDER-TREE-WALK(X.left)                    PREORDER-TREE-WALK(X.right)            POSTORDER-TREE-WALK:  //prints root after subtree nodes                 if x != null                    POSTORDER-TREE-WALK(X.left)                    POSTORDER-TREE-WALK(X.right)                    print X.key        12.2 Querying a binary search tree 289            BST-SEARCH(x, k):                if x != null                    if x.key == k                        return x                    else if x.key > k                        return BST-SEARCH(x.left, k)                    else if x.key < k                        return BST-SEARCH(x.right, k)                return x                        ITERATIVE-BST-SEARCH(x, k):                while(x != null and x.key != k)                    if x.key > k                        x = x.left                    else if x.key < k                        x = x.right                return x            BST-MIN(x):                if x == null or x.left == null:                    return x                else                    BST-MIN(x.left)            ITERATIVE-BST-MIN(x):                if x == null:                    return x                while(x.left != null):                    x = x.left                return x            (max is the same thing.)            BST-MAX(x):
                if x == null or x.right == null:
                    return x
                else
                    BST-MAX(x.right)

            ITERATIVE-BST-MAX(x):
                if x == null:
                    return x
                while(x.right != null):
                    x = x.right
                return x                        - successor and predecessor follow same logic            - (1) successor of a node with node.right != null is the left-most leaf of its right subtree            - (2) successor of a node with node.right == null is the lowest ancestor of this node such that this node is not contained                in that ancestor's right subtree                - watch for rightmost tree node with node.right=null. has no successor!            - predecessor follows opposite and analogous logic            BST-SUCCESSOR(x):                if x.right != null                    return BST-MIN(x.right)                else                    y = x.p                    while y != null and y.key < x.key  //if keys are not distinct, doesn't work.                        y = y.p                           //thats why its garbage. use books.                    return y            BOOK-TREE-SUCCESSOR(x):                if x.right != null                    return BST-min(x.right)                y = x.p                while y != null and x == y.right    //handles the nondistinct case perfectly.                    x = y                    y = y.p                return y            - complexity of SUCCESSOR, PREDECESSOR, SEARCH, MAX, MIN are all O(H), H = tree height = LG N
        12.3 Insertion and deletion 294            - these operations change the ordering and dynamics of the tree. Not simple. =(            - actually, insert is really simple. added as a leaf always.                - goes all the way down the correct comparing path just to preserve the BST property.                 - has y be the trailing pointer/node so when x becomes null, y will connect to z.            BST-INSERT(T, z)                y = NULL                x = T.root                while x != NULL:                    y = x                    if z.key < x.key                        x = x.left                    else if z.key > x.key                        x = x.right                if y == NULL                    T.root = z                else if z.key < y.key                    y.left = z                else if z.key > y.key                    y.right = z            - deletion is not simple. there are three cases for deleting a node z from BST T                (1) if z has no children, then you just remove it from the tree, replacing its                    parent's child pointer to z (left or right) to null                (2) if z only has one child (and therefore, only one possible subtree of descendants),                    we connect the parent of z's child pointer that was originally pointing to z to                    the single child of z                (3) z has two children - z's successor, we call y, must be in z's right subtree                    (3i) if y is z's immediate right child, then we just connect z's parent to y                    (3ii) if not, then we have to replace y with its own right child, then replace z with y                - all this does is point the pointer of u's parent that currently points to u,                     to now point to v.                - does NOT handle child reconnections                TRANSPLANT(T, u, v):                    if u.p == NULL      //u is the root node and has no parent. make v root.                        T.root = v                    else if u == u.p.left       // connects new parent to new node                        u.p.left = v                    else if u == u.p.right                        u.p.right = v                    if v != NULL        // connects new node to new parent                        v.p = u.p                BST-DELETE(T, z)                    // z has no kids                    if z.left == null and z.right == null:                        TRANSPLANT(T, z, NULL)                    // z has one kid                    else if z.left == null:                        TRANSPLANT(T, z, z.right)                    else if z.right == null:                        TRANSPLANT(T, z, z.left)                    // z has two kids                    else:                        //get z's successor                        y = BST-SUCCESSOR(z)  //maybe this doesnt work here? SHOULD be same                        // if y is z's immediate child - you have to do this step anyway...                        if y.p == z:                            TRANSPLANT(T, z, y)                            y.left = z.left                            y.left.p = y                        // otherwise replace y with own right child, then replace z with y                        else:                            TRANSPLANT(T, y, y.right) //no extra connections needed down there!                            y.right = z.right                             y.right.p = y                            TRANSPLANT(T, z, y)
                            y.left = z.left
                            y.left.p = y                BOOK-TREE-DELETE(T, z)                    if z.left == null                        TRANSPLANT(T, z, z.right)                    else if z.right == null                        TRANSPLANT(T, z, z.left)                    else                         y = BST-MINIMUM(z.right)                        if y.p != z                     // when y is not child of z                            TRANSPLANT(T, y, y.right)   //'DOWN THERE': first need to connect y.p with y.right                            y.right = z.right           //'UP HERE': y needs new right kid, point to z's kid                            y.right.p = y               //'UP HERE': finishing up just last statement, point kid to y.                        TRANSPLANT(T, z, y)     //needs to be done ALWAYS: connect z's parent to y                        y.left = z.left         //connect y and z's old left to each other.                        y.left.p = y                                        - complexity is O(H) = O(LG N) due to calling BST-MIN or BST-SUCCESSOR. All else is O(1) in DELETE                        
        12.4 Randomly built binary search trees 299
    13 Red-Black Trees 308
        13.1 Properties of red-black trees 308
        13.2 Rotations 312
        13.3 Insertion 315
        13.4 Deletion 323
    14 Augmenting Data Structures 339
        14.1 Dynamic order statistics 339
        14.2 How to augment a data structure 345
        14.3 Interval trees 348
        
IV Advanced Design and Analysis Techniques
    Introduction 357
    15 Dynamic Programming 359    - the 'programming' in dynamic programming refers to tabularization. (not memoization =))    - applies for divide and conquer problems when the subproblems have overlapping subsolutions    - frequently applied to OPTIMIZATION problems. wanna find OPTIMAL solution not just a soln    STEPS:        (1) characterize structure of optimal solution        (2) recursively define value of optimal solution        (3) compute value of optimal solution, typically in BOTTOM-UP FASHION (tabularization)        (4) construct optimal solution from computed information    - sections follow use DP to solve SPECIFIC examples of OPTIMIZATION problems.    
        15.1 Rod cutting 360            - have rod of length n. want to know optimal way to cut it to get most revenue, given a            table of prices of rods of lengths [1..n].                                 
        15.2 Matrix-chain multiplication 370
        15.3 Elements of dynamic programming 378
        15.4 Longest common subsequence 390
        15.5 Optimal binary search trees 397
    16 Greedy Algorithms 414
        16.1 An activity-selection problem 415
        16.2 Elements of the greedy strategy 423
        16.3 Huffman codes 428
        16.4 Matroids and greedy methods 437
        16.5 A task-scheduling problem as a matroid 443
    17 Amortized Analysis 451
        17.1 Aggregate analysis 452
        17.2 The accounting method 456
        17.3 The potential method 459
        17.4 Dynamic tables 463

V Advanced Data Structures
    Introduction 481
    18 B-Trees 484
        18.1 Definition of B-trees 488
        18.2 Basic operations on B-trees 491
        18.3 Deleting a key from a B-tree 499
    19 Fibonacci Heaps 505
        19.1 Structure of Fibonacci heaps 507
        19.2 Mergeable-heap operations 510
        19.3 Decreasing a key and deleting a node 518
        19.4 Bounding the maximum degree 523
    20 van Emde Boas Trees 531
        20.1 Preliminary approaches 532
        20.2 A recursive structure 536
        20.3 The van Emde Boas tree 545
    21 Data Structures for Disjoint Sets 561
        21.1 Disjoint-set operations 561
        21.2 Linked-list representation of disjoint sets 564
        21.3 Disjoint-set forests 568
        21.4 Analysis of union by rank with path compression 573
    
VI Graph Algorithms
Introduction 587
    22 Elementary Graph Algorithms 589
        22.1 Representations of graphs 589            - need to represent a graph G = (V, E), V set of vertices in graph, E set of edges in graph            - both representations work well for directed and undirected graphs.
            (1) collection of ADJACENCY LISTS REPRESENTATION                PROS                    - good for representing SPARSE graphs, where |E| <<< |V|^2                    - usually better b/c of it, book assumes this representation                CON                    - hard to figure out without parsing a list if two vertices are connected                DESCRIPTION                - Consists of an array A of length |V|, where each entry represents each vertex in V                - for every u in V, A[u] contains a linked list whose nodes are the vertices                     in V that u has an edge to                    - the sum of lengths of all the lists in A is |E| for a directed graph                        - in an undirected graph, this sum is 2|E|                - MEMORY NEEDED: O(V + E)                - EDGE WEIGHTS can be stored at each node (that represents a vertex) in the LL                     alongside with the vertex label            (2) ADJACENCY MATRIX                PROS                    - good for representing DENSE graphs, where |E| is close to |V|^2                    - good for when need to quickly tell if there is an edge b/w two vertices                    - simpler, better when small graphs.                    - require only 1 bit per entry on unweighted graphs.                CON                    - lots of memory. redundant information past diagonal in undirected graph.                DESCRIPTION                - assume vertices in G are numbered from 1, 2, .., |V|. This representation of                     a graph G consists of a |V| x |V| matrix A = (a_ij) s.t. a_ij = 1 if there                     is an edge from vertex i to vertex j, and a_ij = 0 otherwise.                 - MEMORY NEEDED: O(V^2)                - EDGE WEIGHTS are easily implemented by replacing 1 for w in the matrix. 
        22.2 Breadth-first search 594            - BFS takes in a SOURCE vertex s, a graph G = (V, E), and explores edges of G to                 (I) 'discover' every vertex in G that is reachable from s, (II)computing the shortest                 distance b/w each of these vertices from s, (III) as well a "breadth-first tree" with                root s containing all reachable vertices, (IIIa) which contains the simple paths from                 root to every vertex reachable, correspoonding to the shortest path with the                 least number of edges            - BFS labels vertices as (1) not discovered/white, (2) found/gray, (3) totally discovered/black,                where 2 is a node that has been found but has adjacent white vertices, and 3 is a node that                doesn't have anymore white adjacent vertices. This is how BFS keeps track of progress.            - in tree construction, s is the root of the tree, and children are added when the first time a                vertex shows up (aka goes from white to gray). whatever showed up first is the predecessor            BFS(G, s)                //go through every other vertex and init as inf far away, not discovered, no parent yet                for each vertex u in (G.V - {s})                    u.color = 'white'                    u.d = Inf                    u.p = NULL                //set init the source vertex's vals - dist to self is 0, 'found', no parent.                s.color = 'gray'                s.d = 0                s.p = NULL                //init queue of gray vertices to process, and put in the source                Q = queue()                ENQUEUE(Q, s)                //go through every queue item, adding adjacent white nodes to queue and doing stuff                while Q.length != 0                    u = DEQUEUE(Q)                    for each v in G.Adj[u]                        if v.color == 'white'                            v.color = 'gray'                            v.d = u.d + 1                            v.p = u                             ENQUEUE(Q, v)                    u.color = 'black'                - BFS complexity: each vertex is queued and dequeued exactly once. O(V) for queue.                    - for loop goes through each adjacency list for each vertex. sums of lengths                        of all adjacency lists is |E|, so O(E)                    - initialization overhead is O(V) (we set all nodes to white and s to gray, etc)                    - overall, complexity then is O(2V + E) = O(V + E). run in linear time to                         the size of adj list representation of G                                -(proof that BFS computes shortest paths between two vertices)                - the PREDECESSOR SUBGRAPH of G is a BREADTH FIRST TREE if the vertices of the subgraph                    consists of the vertices reachable from s, and there is a unique simple path from s                     to v corresponding to the same shortest path in the original graph G.                    - |E| = |V| - 1 in the breadth first tree.                PRINT-PATH(G, s, v)                    if v == s                        print s                    else if v.p == null                        print 'no path from' s 'to' v 'exists'                    else                        PRINT-PATH(G, s, v.p)                        print v
        22.3 Depth-first search 603            - good for finding cycles? lol            - d is time node was visited initially (white to gray)            - f is time when node was done being processed (gray to black)            - p is still parent node that discovered this node first            1. go through and initialize every vertex to white and parent to null and global time            2. go through each vertex and visit it if it has not been discovered/finished            3. mark this vertex as discovered and record timestamp            4. go through all vertices which this vertex has an edge to RECURSIVELY            5. after doing all of this vertices neighbors recursively, then increment time and mark finished            6. go onto the next vertex that hasnt been discovered yet (should only happen if not connected)            O(V + E) complexity            DFS(G)                for each vertex u in G.V                    u.color = 'white'                    u.p = null                time = 0                for each vertex u in G.V                    if u.color == 'white'                        DFS-VISIT(G, u)                        DFS-VISIT(G, u)                time = time + 1                u.d = time                u.color = 'gray'                for each v in G.Adj[u]                    if v.color == 'white'                        v.p = u                        DFS-VISIT(G, v)                u.color = 'black'                time = time + 1                u.f = time                                - PARENTHESIS STRUCTURE: two depth first paths for a vertex when it is discovered to                 when it is finished are either (1) entirely disjoint, (2 & 3) or one path is completely                contained within the other.                    - v is a descendant and thus nested in u IFF u.d < v.d < v.f < u.f            - v is a descendant of u IFF there exists a path from u to v of only white vertices                at time u.d (when u is discovered)            - a directed graph is acyclic IFF a DFS yields no 'back edges'            (1) TREE EDGES are edges in the DFS forest/tree generated from DFS            (2) BACK EDGES are self loops and edges connecting descendant to ancestor in the DFS tree            (3) FORWARD EDGES are nontree edges connecting a ancestor to descendant in DFS tree            (4) CROSS EDGES are all other edges, such as those b/w different DFS trees, and those that                 are between two vertices that are not ancestor/descendant of one another.            - when were are performing DFS and are on a source node u, going through an edge to a sink                node v, the edge we cross is                 (1) a TREE EDGE if v is currently white/undiscovered                (2) a BACK EDGE if v is currently gray/discovered                (3) and if v is black/finished can be either a FORWARD or a CROSS edge            - in an UNDIRECTED GRAPH, only TREE and BACK edges occur - FORWARD and CROSS do NOT
        22.4 Topological sort 612            - applies only to a DAG or DIRECTED ACYCLIC GRAPH, which is a linear ordering of all                the vertices of the DAG G s.t. if G contains an edge (u,v) then u appears before                v in the ordering.                 - no linear ordering possible if graph has a cycle            TOPOLOGICAL-SORT(G)                DFS(G) to get all finishing times v.f for all vertices v in G.V                as each vertex is finished, insert into front of linked list                return linked list of vertices            costs O(V + E) since DFS takes O(V + E) time and adding to a LL takes O(1) time            - a directed graph G is acyclic IFF a DFS of G yields no back edges      
        22.5 Strongly connected components 615            - DFS good for decomposing a directed graph into its strongly connected components            - a STRONGLY CONNECTED COMPONENT of a directed graph G = (V, E) is a maximimal set                of vertices C, which is a subset of V, s.t. for each pair of vertices (u,v) in C                we have that both u->v and v->u. Define x->y to mean vertex y is reachable from x                - essentially the set of sets of all vertex pairs of G that have paths to each other            - the transpose of a directed graph just reverses all the edge directions in the graph                - two vertices are strongly connected in G IFF they are reachable in G and G_transpose                        STRONGLY-CONNECTED-COMPONENTS(G)                DFS(G) to get all finishing times u.f for all vertices u in G.V                compute G_transpose                DFS(G_transpose) - but consider the vertices in order of decreasing u.f                output vertices of each tree in the DFS forest from previous line as a strongly connected component                    costs O(3V + 3E) cuz 2x DFS of V + E, and G_transpose compute costs V+E            - given two distinct strongly connected components C1, C2, of directed G, and vertices u1, v1                 of C1 and u2, v2 of C2 - if there exists u1->u2, then there cannot be v2->v1            - given two distinct strongly connected components C1, C2 of directed G. If there                is an edge (u,v) s.t. u in C1 and v in C2, then f(C1) > f(C2)                - if there is an edge from u to v in G_transpose, then f(C1) < f(C2)
    23 Minimum Spanning Trees 624
        23.1 Growing a minimum spanning tree 625
        23.2 The algorithms of Kruskal and Prim 631
    24 Single-Source Shortest Paths 643        - given a weighted, directed graph G, and a weight function w mapping edges to real-valued            weights, the weight w(p) of a path (v0, v1, ..., vk) is the sums of the weights of the            edges inside the path.        - we define the SHORTEST-PATH WEIGHT delta(u, v) from u to v to be w(p), where p is the path            in the set of paths from u to v P s.t. w(p) yields the smallest value when compared to             all other paths p_o in P that yield a higher value w(p_o), or infinity if P is empty.            - the SHORTEST PATH from vertex u to v is then the path with weight w(p) = delta(u,v)        - the SINGLE SOURCE SHORTEST PATH problem we want to find the shortest path from EVERY other             vertex in G to the source vertex specified s            - SINGLE PAIR SHORTEST PATH is the same just for one pair of vertices            - can flip edges to do SINGLE DESTINATION SHORTEST PATH.             - ALL PAIRS SHORTEST PATH is the the same just for ALL vertices        - shortest paths CANNOT contain a cycle (positive or negative weight).        INITIALIZE-SINGLE-SOURCE(G, s)      //inits all vertex dist to inf b/c havent started yet            for each vertex v in G.V                v.d = infinity                v.p = null            s.d = 0        RELAX(u, v, w)                      //decreases the dist when we've seen a smaller dist            if v.d > u.d + w(u, v)              //current node is u in algorithm.                v.d = u.d + w(u, v)                v.p = u
        24.1 The Bellman-Ford algorithm 651
        24.2 Single-source shortest paths in directed acyclic graphs 655
        24.3 Dijkstraâ€™s algorithm 658            --- ASSSUMES that w(u,v) >= for all edges (u,v) in a directed, weighted G.E            -> solves single-source shortest path problem. gets shortest path b/w s & each/all v's            (1)init dist to source self to 0 & to all other vertex (havent started yet!) to inf            (2)init S and Q s.t. set-wise, Q = V - S as a loop invariant. Q is min-priority queue            (3)going through vertices in Q                (3a) remove the vertex u with smallest d, and put into S. u=s initially                (3b) for each vertex v connected to u, recalculate v.d             DIJKSTRA(G, w, s)                INITIALIZE-SINGLE-SOURCE(G, s)                                  S = empty-set()                Q = G.V                while Q.length != 0                    u = EXTRACT-MIN(Q)                  //note u = s 1st b/c s.d=0 & v.d = inf                    S = S.append(u)                    for each vertex v in G.Adj[u]                               RELAX(u, v, w)            - The A* algorithm is the SAME as dijkstra's, but it also has another function h in                addition to the weight function w that contributes to the distance of a node from                the source node. h is a heuristic function which contributes a secondary term for                the total weight, but depends on the problem set.                 - DIJKSTRA is the SAME as A*, but with a heuristic function of h(v) = 0.             A* PATHFINDING(G, w, h, s)                INITIALIZE-SINGLE-SOURCE-A*(G)       //also sets s.fs = 0 and v.fs = inf for all v                   S = empty-set()                Q = G.V                while Q.length != 0                     u = EXTRACT-MIN-FS(Q)           // picks next one based off f-score, not dist                    S.append(u)                    for each vertex v in G.Adj[u]                        RELAX-FS(u, v, w, h)        // updates v.fs for all in addition to usual v.d                
        24.4 Difference constraints and shortest paths 664
        24.5 Proofs of shortest-paths properties 671
    25 All-Pairs Shortest Paths 684
        25.1 Shortest paths and matrix multiplication 686
        25.2 The Floyd-Warshall algorithm 693
        25.3 Johnsonâ€™s algorithm for sparse graphs 700
    26 Maximum Flow 708
        26.1 Flow networks 709
        26.2 The Ford-Fulkerson method 714
        26.3 Maximum bipartite matching 732
        26.4 Push-relabel algorithms 736
        26.5 The relabel-to-front algorithm 748

VII Selected Topics
    Introduction 769
    27 Multithreaded Algorithms 772
        27.1 The basics of dynamic multithreading 774
        27.2 Multithreaded matrix multiplication 792
        27.3 Multithreaded merge sort 797
    28 Matrix Operations 813
        28.1 Solving systems of linear equations 813
        28.2 Inverting matrices 827
        28.3 Symmetric positive-definite matrices and least-squares approximation 832
    29 Linear Programming 843
        29.1 Standard and slack forms 850
        29.2 Formulating problems as linear programs 859
        29.3 The simplex algorithm 864
        29.4 Duality 879
        29.5 The initial basic feasible solution 886
    30 Polynomials and the FFT 898
        30.1 Representing polynomials 900
        30.2 The DFT and FFT 906
        30.3 Efficient FFT implementations 915
    31 Number-Theoretic Algorithms 926
        31.1 Elementary number-theoretic notions 927
        31.2 Greatest common divisor 933
        31.3 Modular arithmetic 939
        31.4 Solving modular linear equations 946
        31.5 The Chinese remainder theorem 950
        31.6 Powers of an element 954
        31.7 The RSA public-key cryptosystem 958
        31.8 Primality testing 965
        31.9 Integer factorization 975
    32 String Matching 985
        32.1 The naive string-matching algorithm 988
        32.2 The Rabin-Karp algorithm 990
        32.3 String matching with finite automata 995
        32.4 The Knuth-Morris-Pratt algorithm 1002
    33 Computational Geometry 1014
        33.1 Line-segment properties 1015
        33.2 Determining whether any pair of segments intersects 1021
        33.3 Finding the convex hull 1029
        33.4 Finding the closest pair of points 1039
    34 NP-Completeness 1048        - polynomial time algorithms are those whose worst-case running time on an            input of size n is O(n^k) for some constant k            - generally, these are 'easy'                - problems that cant be solved in polynomial time (but can be solved)                    are 'hard'        - NP-COMPLETE PROBLEMS are problems whose status are unknown. No polynomial            time algorithm has been discovered that solves it, NOR has anyone been            able to prove that no polynomial time algorithm can EXIST for any of them        - P: problems that are solvable in polynomial O(n^k) time, for k const and             n the input size to the problem.        - NP: problems that are 'verifiable' in polynomial time. AKA, given a problem            and an EXAMPLE SOLUTION to the problem (but NOT an ALGORITHM to get to a             solution of the problem), we can verify that the example solution is a             solution to the problem in polynomial time.            - P is a subset of NP, b/c we have algorithms that run in polynomial time                that solve the P problem -> then, it must be polynomial time bounded                to verify an example output to the algorithm is indeed a solution                 - (it should obviously take less time to verify a solution is correct                     rather than to create a solution. SHOULD.....)            - however, is P a PROPER subset of NP? or does P = NP?        - NP-complete or NPC: problems that are in the set of NP and 'as hard as any            problem in NP'            - PROPERTY of the NPC: if ANY problem in NPC can be solved in polynomial                 time, then EVERY problem in NPC is solvable in polynomial time.            - its theorized that NP-complete problems have no efficient solns.                - useful to prove a problem is NPC, and then go make an approx algo.        - NP-hard or NPH: problems that are hard to solve. NP-complete is the set             of NP-hard problems that have solutions that are verifiable in polynomial            time. 
        34.1 Polynomial time 1053
        34.2 Polynomial-time verification 1061
        34.3 NP-completeness and reducibility 1067                    
        34.4 NP-completeness proofs 1078
        34.5 NP-complete problems 1086                        - CLIQUE problem            - VERTEX-COVER problem            - HAMILTONIAN-CYCLE problem            - TRAVELING-SALESMAN problem            - SUBSET-SUM problem
    35 Approximation Algorithms 1106
        35.1 The vertex-cover problem 1108
        35.2 The traveling-salesman problem 1111            - given a complete undirected graph G = (V,E) with a nonnegative integer                cost c(u,v) associated with each edge (u,v) in E, we want to find                a hamiltonian cycle (a tour) of G with minimum cost.             - A hamiltonian cycle is a path through a graph that visits every vertex                exactly once that that is a cycle/closed loop/starts @ same vertex
        35.3 The set-covering problem 1117
        35.4 Randomization and linear programming 1123
        35.5 The subset-sum problem 1128